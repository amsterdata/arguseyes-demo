{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrospective analysis of data leakage in a price prediction pipeline\n",
    "\n",
    "This example evolves around an [ML pipeline for predicting the price of taxi rides](https://github.com/schelterlabs/arguseyes-example/blob/main/pipelines/mlflow-regression-nyctaxifare.py), based on a sample from the [New York City Taxi Fare Prediction](https://www.kaggle.com/competitions/new-york-city-taxi-fare-prediction/data) dataset. The pipeline computes additional features, splits the data into train and testset based on date information, and learns a **regression model to predict the fare of a ride**, based on attributes such as the **pickup time**, **dropoff time**, **trip_distance** and the **zip codes** of the pickup and dropoff locations.\n",
    "\n",
    "When screening this pipeline on Github with this [configuration](https://github.com/schelterlabs/arguseyes-example/blob/main/mlflow-regression-nyctaxifare-dataleakage.yaml), ArgusEyes detects a **data leakage problem** in the pipeline. The screenshot shows the result of the [screening during the build triggered by a Github action](https://github.com/schelterlabs/arguseyes-example/actions/runs/3523396218/jobs/5907507086): There are **177 input tuples which leaked from the train set to the test set**.\n",
    "\n",
    "In the following, we show how to **leverage ArgusEyes to retrospectively analyze the pipeline run** (based on metadata and captured data artifacts), and **figure out the root cause of the data leakage issue**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data-leakage-screening-via-a-github-action](github-action-dataleakage-screening.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the metadata and artifacts from the original run of the pipeline\n",
    "\n",
    "ArgusEyes needs the run id from the mlflow run where ArgusEyes stored the metadata and artifacts. (Note we use a local run here for demo purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arguseyes.retrospective import PipelineRun, DataLeakageRetrospective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = 'b490f97b50244876b6d7b9ec89af6dbf'\n",
    "\n",
    "run = PipelineRun(run_id=run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactively explore the dataflow plan and data of the pipeline run\n",
    "\n",
    "We can view a dataflow plan of the pipeline, which highlights the input datasets, as well as the features and labels for the train and test data computed by the pipeline. We can interactively explore the pipeline data. Clicking on the pink data vertices provides us with details about the corresponding data.\n",
    "\n",
    "<span style='color: red;'>[Note that this interactive widget is only shown during the actual execution of the notebook with jupyter and not rendered in the offline view on Github.]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Pipeline Data Explorer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329a15a5c42548deb8083c0d74843969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(CytoscapeWidget(cytoscape_layout={'name': 'dagre'}, cytoscape_style=[{'selector': 'node', 'css'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.explore_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrospective analysis of the data leakage issue\n",
    "\n",
    "ArgusEyes allows us to instantiate a special `DataLeakageRetrospective`, which helps us analyze data leakage problems from a pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrospective = DataLeakageRetrospective(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Materialize leaked tuples\n",
    "\n",
    "We can compute the tuples that were leaked between the train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_zip</th>\n",
       "      <th>dropoff_zip</th>\n",
       "      <th>trip_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-02-15 12:07:32</td>\n",
       "      <td>2016-02-15 12:40:20</td>\n",
       "      <td>38.5</td>\n",
       "      <td>10018</td>\n",
       "      <td>11371</td>\n",
       "      <td>11.547500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-02-15 01:22:13</td>\n",
       "      <td>2016-02-15 01:30:47</td>\n",
       "      <td>8.5</td>\n",
       "      <td>10020</td>\n",
       "      <td>10022</td>\n",
       "      <td>1.207500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-02-14 23:55:18</td>\n",
       "      <td>2016-02-15 00:03:40</td>\n",
       "      <td>8.5</td>\n",
       "      <td>10152</td>\n",
       "      <td>10001</td>\n",
       "      <td>1.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-02-15 13:03:55</td>\n",
       "      <td>2016-02-15 13:19:04</td>\n",
       "      <td>12.5</td>\n",
       "      <td>10017</td>\n",
       "      <td>10021</td>\n",
       "      <td>1.833846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-02-15 21:20:45</td>\n",
       "      <td>2016-02-15 21:27:58</td>\n",
       "      <td>7.5</td>\n",
       "      <td>10012</td>\n",
       "      <td>10119</td>\n",
       "      <td>2.232500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>2016-02-15 16:36:09</td>\n",
       "      <td>2016-02-15 16:49:11</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10003</td>\n",
       "      <td>10119</td>\n",
       "      <td>1.784286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>2016-02-15 13:26:35</td>\n",
       "      <td>2016-02-15 13:36:25</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10001</td>\n",
       "      <td>10020</td>\n",
       "      <td>1.071667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>2016-02-15 20:22:56</td>\n",
       "      <td>2016-02-15 20:48:52</td>\n",
       "      <td>30.5</td>\n",
       "      <td>11371</td>\n",
       "      <td>10103</td>\n",
       "      <td>10.305714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>2016-02-15 14:24:35</td>\n",
       "      <td>2016-02-15 14:37:27</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10025</td>\n",
       "      <td>10019</td>\n",
       "      <td>2.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2016-02-15 21:02:47</td>\n",
       "      <td>2016-02-15 21:05:27</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10035</td>\n",
       "      <td>10029</td>\n",
       "      <td>1.446000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    tpep_pickup_datetime tpep_dropoff_datetime  fare_amount  pickup_zip  \\\n",
       "0    2016-02-15 12:07:32   2016-02-15 12:40:20         38.5       10018   \n",
       "1    2016-02-15 01:22:13   2016-02-15 01:30:47          8.5       10020   \n",
       "2    2016-02-14 23:55:18   2016-02-15 00:03:40          8.5       10152   \n",
       "3    2016-02-15 13:03:55   2016-02-15 13:19:04         12.5       10017   \n",
       "4    2016-02-15 21:20:45   2016-02-15 21:27:58          7.5       10012   \n",
       "..                   ...                   ...          ...         ...   \n",
       "172  2016-02-15 16:36:09   2016-02-15 16:49:11          9.5       10003   \n",
       "173  2016-02-15 13:26:35   2016-02-15 13:36:25          8.0       10001   \n",
       "174  2016-02-15 20:22:56   2016-02-15 20:48:52         30.5       11371   \n",
       "175  2016-02-15 14:24:35   2016-02-15 14:37:27         11.0       10025   \n",
       "176  2016-02-15 21:02:47   2016-02-15 21:05:27          5.0       10035   \n",
       "\n",
       "     dropoff_zip  trip_distance  \n",
       "0          11371      11.547500  \n",
       "1          10022       1.207500  \n",
       "2          10001       1.630000  \n",
       "3          10021       1.833846  \n",
       "4          10119       2.232500  \n",
       "..           ...            ...  \n",
       "172        10119       1.784286  \n",
       "173        10020       1.071667  \n",
       "174        10103      10.305714  \n",
       "175        10019       2.450000  \n",
       "176        10029       1.446000  \n",
       "\n",
       "[177 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaked_data = retrospective.compute_leaked_tuples()\n",
    "leaked_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep dive into leaked tuples\n",
    "\n",
    "In the following, we can explore the leaked tuples in detail in order to find patterns, which help us determine the root cause of the leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    177.000000\n",
       "mean       2.954813\n",
       "std        3.761105\n",
       "min        0.480909\n",
       "25%        1.047273\n",
       "50%        1.470870\n",
       "75%        2.530000\n",
       "max       19.087500\n",
       "Name: trip_distance, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaked_data.trip_distance.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2016-02-15    175\n",
       "2016-02-14      2\n",
       "Name: tpep_pickup_datetime, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaked_data.tpep_pickup_datetime.dt.date.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the root cause of the leakage\n",
    "\n",
    "All the leaked tuples share the same day in their dropoff time! This is a strong hint that the data was not split correctly for train/test. Fixing this will remove the data leakage issue in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2016-02-15    177\n",
       "Name: tpep_dropoff_datetime, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaked_data.tpep_dropoff_datetime.dt.date.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
